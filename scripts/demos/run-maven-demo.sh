#!/bin/bash

# OLLAMA TinyLlama Demo using Maven exec plugin
echo "üöÄ AMCP OLLAMA TinyLlama 1.1B Demo (Maven)"
echo "=========================================="

# Set JAVA_HOME
export JAVA_HOME="/opt/homebrew/Cellar/openjdk@21/21.0.8/libexec/openjdk.jdk/Contents/Home"

# Check OLLAMA service
echo "üîç Checking OLLAMA service..."
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚ùå OLLAMA not running. Starting it now..."
    echo "Run: brew services start ollama"
    echo "Or: ollama serve &"
    exit 1
fi

# Check TinyLlama model
echo "üîç Checking TinyLlama model..."
if ! ollama list | grep -q tinyllama; then
    echo "üì• TinyLlama not found. Downloading..."
    ollama pull tinyllama
    echo "‚úÖ TinyLlama downloaded successfully!"
fi

echo "‚úÖ All prerequisites ready!"
echo ""
echo "üí° TinyLlama 1.1B Specifications:"
echo "   ‚Ä¢ Parameters: 1.1 billion"
echo "   ‚Ä¢ Download Size: ~637MB"
echo "   ‚Ä¢ RAM Usage: ~2-3GB"
echo "   ‚Ä¢ Speed: Very fast (small model size)"
echo "   ‚Ä¢ Trained on: 1+ trillion tokens"
echo "   ‚Ä¢ Perfect for: Basic conversation, lightweight AI tasks"
echo ""
echo "üéØ Available Commands in Demo:"
echo "   ‚Ä¢ Type any message to chat with TinyLlama"
echo "   ‚Ä¢ /help - Show available commands"
echo "   ‚Ä¢ /model - Display model information"
echo "   ‚Ä¢ /exit - Exit the demo"
echo ""
echo "üöÄ Starting OLLAMA Integration Demo..."
echo "====================================="
echo ""

# Run using Maven exec plugin to handle dependencies properly
mvn exec:java \
    -Dexec.mainClass="io.amcp.connectors.ollama.OllamaIntegrationDemo" \
    -Dexec.args="" \
    -pl connectors \
    -q