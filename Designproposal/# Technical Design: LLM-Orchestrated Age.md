# Technical Design: LLM-Orchestrated Agent Collaboration for AMCP v1.5

## Introduction and Objectives  
This design extends **AMCP v1.5** with a **Large Language Model (LLM)-driven Orchestrator Agent** and supporting components to enable **dynamic multi-agent orchestration**. The goal is to allow agents to collaborate on complex tasks by **delegating subtasks** to specialized agents, coordinated by an orchestrator using natural language reasoning. In essence, we introduce an agent (the *Orchestrator*) that can interpret a high-level request (in natural language), break it into discrete actions, assign those actions to appropriate **specialist agents**, and assemble the results into a final answer[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). This leverages an LLM (integrated via **Ollama**, a local LLM server) within the orchestrator to perform planning and synthesis.

**Key features of the new design:**

- **LLM-Powered Orchestrator Agent:** Uses an LLM to analyze user requests and devise a plan of action. It interacts with other agents through AMCP’s event bus (pub/sub), rather than direct calls, ensuring asynchronous and decoupled coordination[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).
- **Agent-to-Agent Tasking Protocol:** Defines a standardized way for agents to request work from each other via events (carrying a *task request* and expecting a *task response*). Uses AMCP's CloudEvents-compliant messages with correlation IDs for matching responses[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).
- **Dynamic Capability Registry:** A mechanism for agents to **advertise their capabilities** (skills) at runtime, and for the orchestrator to discover which agent can handle a given subtask[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). This may be a simple **Registry Agent** in the mesh that tracks active agents and their declared skills.
- **Seamless LLM Integration via Ollama:** The design integrates the **Ollama** LLM server to keep LLM inference on-premise. The orchestrator communicates with Ollama’s HTTP API to get plans and possibly to refine outputs, treating the LLM as just another tool (though a very powerful one) in the workflow.
- **Compatibility with AMCP v1.5 Core:** The design does **not require changes** to the core AMCP platform. It adds agents and uses existing features (CloudEvents, metadata, pub/sub, connectors). Mobile agents and existing event brokers are fully compatible with this orchestrator extension – in fact, the orchestrator pattern builds on AMCP’s strength of asynchronous many-to-many communication.
- **Asynchronous, Non-Blocking Workflow:** The orchestrator dispatches tasks in parallel (when possible) and waits for responses asynchronously, leveraging AMCP’s event-driven nature. This yields a scalable solution where multiple specialist agents can work concurrently on different aspects of a problem, with the orchestrator aggregating results[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). No single agent is ever blocked waiting on another – they synchronize only via events, which avoids deadlocks and allows concurrency.

By implementing this design, AMCP v1.5 can support advanced **agentic AI scenarios**: e.g., a user’s query can trigger a coordinated effort among an ensemble of agents (data retrieval, computation, and content generation agents) orchestrated by the LLM’s reasoning[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). The following sections detail the component design, class responsibilities, pseudocode algorithms, correctness considerations, testing approach, and usage examples.

---

## System Architecture Overview

At a high level, the extended system introduces an **Orchestration Layer** on top of the existing agent mesh. The core AMCP event mesh (broker) connects all agents, including the orchestrator and specialists. A typical interaction flow is:

<table>
<thead>
<tr><th>Actor/Component</th><th>Interaction</th></tr>
</thead>
<tbody>
<tr>
  <td><strong>User / Client</strong></td>
  <td>Issues a high-level request in natural language (e.g., via a UI or API).</td>
</tr>
<tr>
  <td><strong>Gateway Agent (optional)</strong></td>
  <td>Receives the user request, performs authentication/authorization if needed, and packages it into an AMCP <b>request event</b> (CloudEvent) that is published to the orchestrator's input channel. (If no gateway, the request event could be generated by client code or another agent.)</td>
</tr>
<tr>
  <td><strong>Orchestrator Agent</strong></td>
  <td>Subscribing on the request channel, receives the event. Uses the LLM (via Ollama) to analyze the request and plan a set of tasks. It then publishes a <b>task request event</b> for each subtask, targeting the appropriate specialist agent(s). The orchestrator remains awaiting results.</td>
</tr>
<tr>
  <td><strong>Specialist Agent(s)</strong></td>
  <td>Subscribed to relevant task topics (or addressed directly), receive their respective task request events. Each agent performs its task – possibly using external tools (APIs, databases) via connectors – and then publishes a <b>task response event</b> with the results.</td>
</tr>
<tr>
  <td><strong>Orchestrator Agent</strong></td>
  <td>Collects all task responses (correlated to the original request) as they arrive. Once all subtasks are completed (or a timeout/error occurs), the orchestrator composes the final outcome. It may call the LLM again to synthesize a cohesive answer from the partial results. Finally, it publishes a <b>response event</b> back to the requester (e.g., on a reply topic or to the Gateway).</td>
</tr>
<tr>
  <td><strong>Gateway / Client</strong></td>
  <td>Delivers the orchestrator’s response to the end user (for instance, sending a JSON response to an HTTP client or a message back in chat). The cycle is complete.</td>
</tr>
</tbody>
</table>

Conceptually, this is similar to the “Hierarchical Orchestration” pattern of multi-agent systems[2](https://github.com/AIAnytime/Multi-Agents-Orchestration-Design-Patterns)[2](https://github.com/AIAnytime/Multi-Agents-Orchestration-Design-Patterns): a master agent delegates to specialized workers and integrates their outputs. The orchestrator uses the LLM’s reasoning ability to perform flexible planning beyond hard-coded workflows, which is inspired by Microsoft’s *HuggingGPT* and Google’s *ADK* orchestrator models. Meanwhile, all communication remains asynchronous events in the AMCP mesh, ensuring a loosely coupled design where agents can be added or replaced without modifying the orchestrator’s core logic (they just need to register their capabilities).

Below is an **ASCII sequence diagram** summarizing the orchestration flow:

```
User (external)
   | 
   | (1) Query: "High-level Request?"
   v
Gateway Agent (ingress)
   | 
   | (2) --> [Publish Event: user.request{"query": "..."}] 
   v
Orchestrator Agent (planner)
   | (3) Receives request event
   | (4) --> [Calls LLM with request]
   | (5) <-- [LLM returns plan: Tasks A, B, ...]
   | (6) For each Task:
   |        [Publish Event: task.request{task=A, corrId=X}] 
   |        [Publish Event: task.request{task=B, corrId=X}] 
   |         ...
   v
Specialist Agent A       Specialist Agent B        ... 
   | (7) Receives task A   | (7) Receives task B   | 
   |    Executes work      |    Executes work      | 
   | (8) --> [Publishes    | (8) --> [Publishes    | 
   |        task.response{corrId=X, result_A}]    | ...
   v                    v
Orchestrator Agent 
   | (9) Collects responses for corrId X (A's result, B's result, ...)
   | (10) If all tasks done:
   |       [Optionally call LLM to compose final answer]
   |       Prepare final result
   | (11) --> [Publish Event: user.response{corrId=X, answer}]
   v
Gateway Agent 
   | 
   | (12) Forwards answer to user (e.g., HTTP response or chat message)
   v
User 
   | 
   | (13) Receives final answer 
   v
```

Steps (1)–(13) correspond to the flow described earlier. Note that steps (6)–(8) allow parallel execution: multiple specialist agents work simultaneously, and step (9) aggregates their responses asynchronously[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). The **correlationId** (`X` in the diagram) links all events belonging to the same top-level request, enabling the orchestrator to match responses to the right request. 

**Dynamic Agent Discovery:** A new agent joining the system can announce its capabilities to the orchestrator (via the registry), making itself available for future tasks[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). Conversely, if an agent goes offline, the registry can update accordingly. This dynamic service mesh approach ensures the orchestrator always has an up-to-date view of what skills are available in the “agent society” at runtime, fulfilling the *Dynamic Agent Registry* pattern[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).

**Minimal Impact on Existing Agents:** Existing AMCP agents not participating in orchestration remain unaffected. The orchestrator and registry operate on top of the pub/sub infrastructure and do not require changes to core messaging. The specialist agents simply need to adhere to the task request/response convention (which can often be implemented as an extra event handler in the agent). Thus, this feature can be introduced incrementally into an AMCP deployment.

---

## Component and Class Design

This section describes the main classes/modules introduced and their responsibilities. All classes build upon the AMCP v1.5 framework (particularly the `Agent` base class and event messaging API). **Table 1** provides a summary of key components:

<table>
<thead>
<tr><th>Component (Class)</th><th>Responsibility</th><th>Key Functions</th></tr>
</thead>
<tbody>
<tr>
  <td><strong>OrchestratorAgent</strong><br/>(extends AMCP Agent)</td>
  <td>LLM-driven coordinator. Receives high-level request events and orchestrates their fulfillment by delegating to specialist agents. Maintains a view of available agent <em>capabilities</em> to inform task assignment.</td>
  <td>
    <code>handleRequest(event)</code> – Invoke LLM to produce a task plan from a request.<br/>
    <code>dispatchTasks(plan)</code> – Publish task request events to target agents (with correlation IDs).<br/>
    <code>onTaskResponse(event)</code> – Collect one task result; when all collected, compile final result.<br/>
    <code>replyToRequester(result)</code> – Publish the final answer event (or error) to the originator.
  </td>
</tr>
<tr>
  <td><strong>AgentRegistry</strong><br/>(could be an Agent or service module)</td>
  <td>Directory of active agents and their advertised skills. Supports runtime agent discovery: orchestrator queries this to find which agent can handle a given task type. Also helps in load-balancing if multiple agents share a skill.</td>
  <td>
    <code>registerAgent(id, capabilities)</code> – Record a new agent’s skills (invoked on agent startup).<br/>
    <code>findAgent(capability)</code> – Return the ID (or address) of an agent that provides the requested capability. (Could also return multiple agents if needed.)<br/>
    <code>deregisterAgent(id)</code> – Remove agent on shutdown/failure.
  </td>
</tr>
<tr>
  <td><strong>Specialist Agent(s)</strong><br/>(extends AMCP Agent)</td>
  <td>Domain-specific worker agents (unchanged in structure, but now listen for orchestrated task events). Each provides one or more "capabilities" (functions it can perform). Examples: DatabaseQueryAgent, ReportGeneratorAgent, WeatherInfoAgent. They execute tasks delegated by the orchestrator and return results.</td>
  <td>
    <code>onTaskRequest(event)</code> – (Convention) Triggered when a task event for this agent is received. Parses task parameters and performs the work.<br/>
    <code>publishResult(corrId, resultData)</code> – Sends a task response event with the given correlation ID, carrying the result or error.
  </td>
</tr>
<tr>
  <td><strong>LLM Connector</strong><br/>(utility class or AMCP Tool Connector)</td>
  <td>Encapsulates communication with the Ollama LLM service. Allows the orchestrator to send prompts and receive completions (plans or summaries). By abstracting this, the orchestrator’s code remains focused on logic rather than HTTP details.</td>
  <td>
    <code>generatePlan(userQuery, capabilitiesList)</code> – Sends a planning prompt to Ollama (with available capabilities) and returns the parsed plan (e.g., as a list of tasks).<br/>
    <code>generateAnswer(summaryPrompt)</code> – (Optional) Sends a prompt to compose the final answer from partial results.<br/>
    <i>Internally</i>, these use an HTTP POST to <code>http://localhost:11434/api/generate</code> with the chosen model and prompt.
  </td>
</tr>
<tr>
  <td><strong>Gateway Agent</strong><br/>(optional, extends Agent or external service)</td>
  <td>Interface between external clients and the AMCP mesh. Not strictly required by orchestrator logic, but in a real deployment it provides a controlled entry/exit point. Ensures only authorized queries reach the orchestrator and formats responses for clients. Could support HTTP, CLI, or messaging interfaces.</td>
  <td>
    <code>onExternalRequest(input)</code> – Translates a user input (say an HTTP request or console command) into a `user.request` event that the orchestrator subscribes to. Includes metadata like user ID, request ID, etc.<br/>
    <code>onAgentResponse(event)</code> – Intercepts the orchestrator’s `user.response` event and forwards it back to the user (HTTP response, print to console, etc.).<br/>
    <i>Security:</i> Could verify API keys or user tokens before forwarding requests, and tag events with user roles for the orchestrator to enforce access policies.
  </td>
</tr>
</tbody>
</table>

**Table 1:** Summary of new components in the orchestrator extension, with their roles and key operations.

Most of these are implemented in **software (class) form** within the AMCP framework:

- The **OrchestratorAgent** is a subclass of the AMCP `Agent` class, meaning it runs inside an `AgentContext` and uses the normal event handling mechanism. It may utilize multiple internal helper classes (like the LLM Connector or a Plan Parser) but those don’t need to be AMCP agents.
- The **AgentRegistry** could be implemented as a singleton service or as an `Agent` as well. One simple design is to have the orchestrator itself hold the registry data (updated via special events that agents send on startup). Another design is a dedicated Registry Agent that orchestrator queries by sending a lookup event. For this design, we lean towards **Orchestrator-managed registry state** for simplicity: agents broadcast a “join” event with their capabilities, and the orchestrator (listening for those) updates an internal map of capability → agent ID. This avoids an extra network hop on each lookup and keeps orchestrator logic self-contained. (However, a separate RegistryAgent would make sense if we had multiple orchestrators or needed persistence.)
- The **Specialist Agents** are just typical AMCP agents, albeit following a convention to handle `task.request` events. We might provide an abstract base like `TaskAgent` to simplify writing new ones (with a method `handleTask(data)` that child classes implement, and the base class wraps result into a response event), but it’s not strictly required. The convention can be documented: *“subscribe to `<yourCapability>.*` topic or similar, accept events with correlationId, and respond with correlationId preserved.”*
- The **LLM Connector** can be a pure utility class (not an agent, since it’s just used by the orchestrator internally). In AMCP’s architecture, this is analogous to a *Tool Connector:* e.g., v1.5 had connectors for external APIs like DuckDuckGo. We can implement an `OllamaConnector` with a simple HTTP client. For testability, the connector could expose an interface so we can swap in a mock LLM during testing.

**Event Topics & Types:** We will use AMCP’s CloudEvents-based type field to distinguish different event kinds:

- **`user.request`** – incoming high-level requests (data contains the user’s query, and possibly user/context info). Source might be “/gateway” or the user’s ID, and a unique `id` (UUID) identifies the request event[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).
- **`task.request`** – events created by orchestrator to ask an agent to do something. We may structure the `type` further as `task.request.<capability>` (e.g., `task.request.finance.db.query`) or put the capability in the data. Either way, specialist agents recognize tasks meant for them. The **subject/topic** field of CloudEvent or AMCP’s `topic` can be used for routing: e.g., orchestrator publishes on topic `agent/finance/db/query` which the DatabaseAgent subscribed to. For clarity, we can use AMCP’s *topic hierarchy* with capability names (since v1.5 supports hierarchical topics).
- **`task.response`** – events with results from agents. These will carry the same `correlationId` as the originating `user.request` (or a sub-correlation if we choose a different scheme). The orchestrator subscribes to these (filtered by correlationId or by expecting them after dispatch). The `data` section contains the outcome or an error message.
- **`user.response`** – final answer from orchestrator. The `correlationId` matches the original request’s ID so the gateway or caller can relate it. Data contains the answer (could be text, JSON, a file link, etc. depending on the query).

All events use the CloudEvents v1.0 schema that AMCP v1.5 mandates[3](https://amadeusworkplace-my.sharepoint.com/personal/xcallens_amadeus_com/Documents/ailored%20to%20specific%20event.docx?web=1)[3](https://amadeusworkplace-my.sharepoint.com/personal/xcallens_amadeus_com/Documents/ailored%20to%20specific%20event.docx?web=1). For example, a `task.request` event might look like (in JSON form):

```json
{
  "specversion": "1.0",
  "id": "789e4567-e89b-12d3-a456-426614174000",
  "type": "task.request",
  "source": "/agent/Orchestrator1",
  "subject": "finance.db.query", 
  "time": "2025-09-29T16:22:31Z",
  "correlationid": "123e4567-e89b-12d3-a456-426614174999",
  "data": { "sql": "SELECT * FROM Expenses WHERE quarter='Q4';" }
}
```

Here `correlationid` ties the event to a higher-level request, and `subject` indicates the nature of the task (could also be encoded in `type` as `task.request.finance.db.query`). The specialist DatabaseAgent would respond with a `type: "task.response"` event with the same correlationid and data containing the query results. By leveraging CloudEvents fields, we ensure consistency and interoperability – for instance, these events could be monitored by external logging or tracing tools for auditing the orchestration flow[3](https://amadeusworkplace-my.sharepoint.com/personal/xcallens_amadeus_com/Documents/ailored%20to%20specific%20event.docx?web=1)[3](https://amadeusworkplace-my.sharepoint.com/personal/xcallens_amadeus_com/Documents/ailored%20to%20specific%20event.docx?web=1).

**Class Relationships:** The orchestrator is central – it interacts with the LLM connector and registry (if separate). Specialist agents are independent; they only know to listen for certain events. The gateway is at the boundary. There is no tight coupling; communication is through events and the registry’s indirection. This means, for example, that adding a new specialist agent type requires *no code change* in the orchestrator – just ensure it registers its capability so the orchestrator might use it in a plan. This realization aligns with the *Service Mesh for Agents* concept, where the orchestrator or registry can resolve agents based on capabilities at runtime[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).

We now delve into the behavior of these components through pseudocode of their algorithms.

---

## Orchestration Logic and Pseudocode

### Orchestrator Agent Algorithm  
The orchestrator’s behavior revolves around handling a **user request event** and overseeing the lifecycle of that request until completion. We present a simplified pseudocode for the OrchestratorAgent (in a Python-like style for clarity):

```python
class OrchestratorAgent(Agent):
    # Internal state
    capabilities = {}  # Map: capability -> agent_id (registry data)
    pending_tasks = {}  # Map: request_id -> tracking info (e.g., tasks left, partial results)
    llm = OllamaConnector(model="llama2-13b")  # LLM interface (could load model name from config)
    
    def on_activate(self):
        # Subscribe to relevant event types
        self.subscribe("user.request")        # external requests
        self.subscribe("agent.register")      # agents announcing themselves
        self.subscribe("task.response.*")     # all task responses (could also filter by corr id internally)
    
    def handle_event(self, event):
        if event.type == "user.request":
            self._handle_user_request(event)
        elif event.type == "agent.register":
            self._handle_registration(event)
        elif event.type == "task.response":
            self._handle_task_response(event)
        # else ignore other events
        
    def _handle_registration(self, event):
        # An agent joining the mesh sends its capabilities
        data = event.data  # e.g., {"agent_id": "Agent42", "capabilities": ["finance.db.query", "report.generate"]}
        agent_id = data["agent_id"]
        for cap in data["capabilities"]:
            self.capabilities[cap] = agent_id
        # (If multiple agents per capability, could store list and later choose one based on load or round-robin)
    
    def _handle_user_request(self, event):
        req_id = event.id  # unique identifier for this request (corrId for tasks)
        user_query = event.data.get("query")  # assuming the request payload has 'query'
        user_meta = event.data.get("user")    # user info, roles, etc., if provided
        
        # 1. Plan tasks with LLM
        available_skills = [cap for cap in self.capabilities.keys()]
        plan = self.llm.generatePlan(user_query, available_skills)
        # 'plan' could be a list of tasks, where each task = {"capability": X, "params": {...}}
        tasks = parse_plan(plan)
        if not tasks:
            # No tasks returned (LLM didn't understand or request trivial)
            answer_text = self.llm.generateAnswer(f"Answer the query directly: {user_query}")
            return self._send_response(req_id, answer_text)
        
        # 2. Dispatch tasks to agents
        self.pending_tasks[req_id] = {
            "count": len(tasks),
            "results": {},
            "origin_event": event
        }
        for task in tasks:
            cap = task["capability"]
            params = task.get("params", {})
            target_agent = self.capabilities.get(cap)
            if not target_agent:
                # No agent for this skill
                # We could handle by skipping or marking an error result for this task.
                self.pending_tasks[req_id]["results"][cap] = {"error": "No agent for capability"}
                self.pending_tasks[req_id]["count"] -= 1
                continue
            # Form a task.request event (type and correlation set)
            task_event = Event(
                type="task.request",
                source=self.id,
                subject=cap,               # e.g., "finance.db.query"
                correlation_id=req_id,
                data={**params, "origin": user_meta or {}}
            )
            self.publish(task_event, target=target_agent)
            # target=target_agent implies either sending directly to that agent's queue,
            # or we publish on a topic that the target agent subscribed uniquely.
        # (After loop, if count went to 0 (no actual tasks dispatched), we can immediately finalize.)
        if self.pending_tasks[req_id]["count"] == 0:
            return self._finalize_request(req_id)
        # else, wait for responses (control returns to event loop)
    
    def _handle_task_response(self, event):
        # A specialist agent's response to some task
        corr_id = event.correlation_id
        if corr_id not in self.pending_tasks:
            return  # not an active request we're tracking
        # Record the result
        task_result = event.data
        source_cap = event.subject or task_result.get("capability")  # what skill responded
        self.pending_tasks[corr_id]["results"][source_cap] = task_result
        # Decrement count and check if all done
        self.pending_tasks[corr_id]["count"] -= 1
        if self.pending_tasks[corr_id]["count"] <= 0:
            # All expected responses received (or none were expected if tasks list was empty)
            self._finalize_request(corr_id)
    
    def _finalize_request(self, req_id):
        """Compose final answer from collected results and send response."""
        record = self.pending_tasks.get(req_id)
        if not record:
            return
        results = record["results"]
        origin_event = record["origin_event"]
        user_query = origin_event.data.get("query", "")
        # Use LLM to synthesize answer if needed
        answer = ""
        if results:
            # Prepare a prompt with all results summarized for the LLM
            prompt = f"User asked: {user_query}\n"
            for cap, res in results.items():
                prompt += f"{cap} result: {res}\n"
            prompt += "Compose a final answer."
            answer = self.llm.generateAnswer(prompt)
        else:
            # If no results (maybe the LLM had answered directly or no tasks), we might have direct answer
            answer = results.get("direct") if results else "(No result)"
        # Publish the response event
        self._send_response(req_id, answer)
        # Cleanup
        del self.pending_tasks[req_id]
    
    def _send_response(self, req_id, answer_text):
        response_event = Event(
            type="user.response",
            source=self.id,
            correlation_id=req_id,
            data={"answer": answer_text}
        )
        # By design, the source of the original request is expecting the response.
        # If via Gateway, maybe origin_event had a reply-to address or we define a static channel.
        # Simpler: publish to topic user.response or to the gateway specifically.
        self.publish(response_event)
```

*(Pseudocode for OrchestratorAgent logic – not actual library calls, but representative of the flow.)*

A few notes on this pseudocode:

- The orchestrator subscribes to `user.request` events (coming from the Gateway or elsewhere), to `agent.register` (capability publication by agents), and to `task.response` events. The subscription to `task.response.*` (with wildcard) could be broad; in practice, it might subscribe to a more filtered topic if the broker supports it (or it may subscribe to a queue where all task responses are routed to orchestrator as well). Using correlation IDs, it filters out responses that belong to active requests.
- We maintain `pending_tasks` as a dictionary keyed by the top-level request ID. This allows the orchestrator to handle multiple outstanding requests concurrently. Because AMCP calls the agent's `handle_event` sequentially for each event, responses might intermix with new requests sequentially, but we track them separately. If concurrency becomes heavy, we could even spawn a new OrchestratorAgent instance per request (but that’s an advanced extension – here we keep one orchestrator handling multiple in an interleaved fashion).
- The LLM plan is assumed to be returned in a structured form that we can parse (e.g., JSON enumerating tasks). We would craft the prompt to the LLM to respond in JSON for easier parsing. Example prompt to LLM: *“User asks: {query}. You can use these capabilities: {list}. Devise up to N steps with format JSON [{{'capability': ..., 'params': {{...}}}}].”* Then `generatePlan` can parse the JSON. This mitigates free-form parsing issues and serves as a **contract** between the orchestrator and LLM.
- Task dispatch uses `correlation_id = req_id` from the original event. This means all tasks share the same correlation, linking them to the single user request. The orchestrator knows how many tasks it sent (the `count`), so it knows when all expected responses are back. We could alternatively assign each task its own sub-correlation and track those, but using one correlation simplifies aggregation when all tasks are independent. It does assume no other concurrent request uses the same ID (which is guaranteed by unique event IDs in CloudEvents).
- Each `task.request` event is published. In the pseudocode, `self.publish(event, target=agent)` implies either a direct send to that agent or a publish to a topic the agent exclusively listens to. AMCP doesn’t natively have a `target` parameter in publish; rather, each agent would subscribe to a topic. So likely the orchestrator would do something like `self.publish(event, topic="agent."+target_agent_id+".task")` or simply `event.source = orchestrator; event.type="task.request"; event.subject=capability` and publish it to a topic named after the capability. We might define a naming convention: e.g., agent with capability *X* subscribes to topic `"task.request."+X` or just `"X"` if unique. For now, assume the orchestrator can route events to specific agents through the broker (some brokers allow direct addressing, others we simulate by distinct topics).
- Handling of agent registration: A specialist agent on startup would send an `agent.register` event with its ID and capabilities (perhaps the Agent Context can trigger this automatically, or the agent’s `on_activate` could do it). The orchestrator receives this and updates its `capabilities` map[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). If multiple agents share one capability, a simple approach is to let the last one registered overwrite, or maintain a list for round-robin or failover. For simplicity, we store one agent per capability (the assumption being capabilities are distinct roles; if needed, extend to list).
- Error conditions: If no agent exists for a planned capability, we decrement the task count and note an error. When finalizing, the orchestrator could decide what to do with that (maybe include in the final answer that some part couldn’t be done). If an agent responds with an error (perhaps in its data payload), the orchestrator can include that info or try an alternative if available. Robust handling can be added: e.g., if an agent fails to respond within X seconds, orchestrator times out and either retries or marks it as error. Such a timeout could be implemented by scheduling (outside this pseudocode) or by noticing not all responses came after a while.
- After finalizing, the orchestrator publishes `user.response`. Ideally, the gateway (or whoever made the request) is subscribed or will receive events with `correlation_id = req_id`. Another design is to include a `reply_to` address in the request and have orchestrator publish there (the pseudocode uses a generic publish which likely goes to a topic; if gateway listens on `user.response` events or on that correlation specifically via filtering, it will get it). We assume the gateway will catch it because it probably has the original request ID to filter on or it just listens to all `user.response` and matches the user session.

The orchestrator algorithm ensures that for each request, every planned task is dispatched and accounted for, and a single final response is emitted. **It operates much like an asynchronous workflow engine, with the twist that the workflow is dynamically generated by an LLM.**

### Specialist Agent (Task Handler) Behavior  
A specialist agent (say a DatabaseAgent) is straightforward. It subscribes to the task topics relevant to its skill. For example, a DatabaseAgent might subscribe to `finance.db.query` tasks. Once it receives one, it does the work and replies:

```python
class DatabaseAgent(Agent):
    def on_activate(self):
        self.subscribe("task.request.finance.db.query")
    def handle_event(self, event):
        if event.type == "task.request" and event.subject == "finance.db.query":
            sql_query = event.data.get("sql")
            try:
                result_rows = self.database.run_query(sql_query)
                response_data = {"rows": result_rows}
            except Exception as e:
                response_data = {"error": str(e)}
            # Publish task.response with same correlation
            response_event = Event(
                type="task.response",
                source=self.id,
                subject="finance.db.query",  # echo capability or could omit
                correlation_id=event.correlation_id,
                data=response_data
            )
            self.publish(response_event, topic="task.response.finance.db.query")
            # We could also broadcast to a generic "task.response" topic; orchestrator is filtering by correlation anyway.
```

Important aspects:
- It uses `event.correlation_id` from the incoming task to tie its response to the right orchestration. The orchestrator set this to the user request ID.
- It retains the `subject` (capability) in the response for clarity, though not strictly necessary. The orchestrator might use it when aggregating results.
- The event is of type `task.response`. We might not need a specific topic for each response type, since the orchestrator is grabbing by correlation. However, for debugging or monitoring, having topics like `task.response.finance.db.query` could be useful. Alternatively, the agent could simply do `self.publish(response_event)` with the same type and rely on the broker delivering it to any subscriber of `task.response` (or a wildcard). In AMCP, using a wildcard subscription on the orchestrator for `task.response.*` covers this.

Each specialist agent will have similar structure: do work, publish result. If a task itself requires interacting with an external tool (e.g., a WeatherAgent calling a weather API), it may use an HTTP connector or MCP integration. That doesn’t affect orchestration logic except for potential delays or errors which the agent should handle.

**Parallelism:** Since each agent runs in its own context/thread, tasks are truly done in parallel if on different agents. Even if multiple tasks went to the same agent, AMCP’s model processes events sequentially per agent (unless that agent explicitly creates threads). To exploit parallelism, the orchestrator should ideally split tasks among different agents. The LLM planning should consider that – e.g., not assign two independent tasks to the same capability agent if we have a way to distribute. If one agent has to do two tasks serially, that’s okay too, just slower (the orchestrator will wait).

### LLM Connector Usage  
We won’t detail the HTTP code, but conceptually:

```python
class OllamaConnector:
    def __init__(self, model="llama2", base_url="http://localhost:11434"):
        self.model = model
        self.url = base_url + "/api/generate"
    def generatePlan(self, user_query, capabilities_list):
        prompt = make_planning_prompt(user_query, capabilities_list)
        response = http_post(self.url, json={"model": self.model, "prompt": prompt})
        plan_text = response.json().get("output")  # or similar, depending on API
        return plan_text  # Or parse JSON from it
    def generateAnswer(self, compose_prompt):
        # Similar idea: call LLM to compose final answer
        response = http_post(self.url, json={"model": self.model, "prompt": compose_prompt})
        return response.json().get("output")
```

We assume Ollama returns the model’s output in one go (since we likely use a smaller model or short prompt). If needed, streaming from Ollama could be handled, but to keep the orchestrator simpler, synchronous calls as above are fine (the agent’s event loop stalls during the call, but that’s acceptable if the model is reasonably fast or if orchestrator is scaled out when needed).

### Communication Protocol Summary

To ensure clarity, here’s how agents interact in terms of the **Agent-to-Agent (A2A) protocol** implemented via events:

- **Discovery**: Agent sends `agent.register` (with its ID & capabilities) → Orchestrator updates its registry[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).
- **Request**: Orchestrator sends `task.request` (with correlationId and task data) → Specialist agent(s) receive it[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).
- **Response**: Specialist sends `task.response` (with same correlationId and result data) → Orchestrator receives it[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).
- **Completion**: Orchestrator sends `user.response` (with correlationId and final answer) → Gateway or requester receives it.

All messages are **one-way events** (fire-and-forget), fitting AMCP’s asynchronous design. However, together they form a logical request/reply chain. We rely on **unique correlation IDs** to pair the replies with the original request context, a common practice in message-based orchestration.

This loosely-coupled protocol offers flexibility: if in the future we want agents to sometimes bypass the orchestrator and call each other directly (for efficiency), they can follow the same pattern (one agent publishes a `task.request` that another handles). The orchestrator could even approve direct agent-to-agent communications for certain pairs, as noted in multi-agent communication pattern references[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html). But initially, keeping the orchestrator in the loop for all tasks simplifies sequence control and logging.

## Correctness and Safety Analysis

We now reason about the design’s correctness (it does what it’s supposed to do) and its safety (absence of harmful behaviors like deadlock, resource leaks, etc.). While a formal proof can be complex due to the involvement of an LLM (which is non-deterministic), we can state key invariants and argue informally:

**1. Guaranteed Completion (Liveness):** Each user request processed by the OrchestratorAgent will eventually produce one of the following outcomes: a final response event with an answer, or a final response event indicating failure. The orchestrator’s logic ensures it doesn’t get stuck waiting indefinitely:
- A request spawns a finite number *N* of task events. We assume the LLM produces a finite plan (in practice, we can cap the number of tasks it can suggest).
- For each task event, the orchestrator expects at most one corresponding response event. If a response never comes (e.g., agent died or network failure), we will implement a **timeout** strategy (not shown in pseudocode, but easy to add: e.g., a scheduled check after T seconds that finalizes the request with missing tasks marked as failed). Thus, *no infinite wait.* 
- When all responses have been accounted for (or timed out), the orchestrator calls `_finalize_request` exactly once, sending exactly one `user.response`. After that, the request `req_id` is removed from `pending_tasks`, so resources are freed and it will not be revisited. This prevents any possibility of multiple responses or none at all – precisely one per request is delivered, satisfying the typical request/reply contract.
- The system avoids *deadlock* because there is no circular waiting: the orchestrator might wait for agent responses, but those agents don’t wait for anything from orchestrator except the initial instruction (which they already got). Agents independently perform tasks and reply. No agent is waiting on another agent’s result (unless the LLM explicitly encoded a dependency in the plan, but in that case the orchestrator would schedule them sequentially).
- Because communication is asynchronous, even if an agent fails mid-task, other agents and the orchestrator are not blocked – they progress or time out accordingly. Use of timeouts and error checks ensures the orchestrator always concludes a request path. This addresses *liveness*: every request triggers a termination condition eventually.

**2. No Missed or Duplicate Correlations (Safety of message handling):** The use of correlation IDs and tracking structures ensures that:
- The orchestrator only aggregates responses that belong to an active request it knows. In `_handle_task_response`, we ignore any event with a correlation that’s not in `pending_tasks`. This could happen if an agent responds very late to an already finalized request or some stray event – orchestrator will drop it. So, we won’t accidentally mix responses between different sessions.
- By setting correlation on task events to the originating request ID, we naturally group them. We assume uniqueness of `event.id` per request (AMCP uses UUIDs for event IDs, so collisions are practically nil). Thus, no two active orchestrations share a correlation ID. Even if orchestrator runs requests sequentially, this holds; if parallel, it’s crucial and is satisfied by UUID nature.
- Each task is sent exactly once (or not at all, if agent missing). We don’t resend unless a retry policy is implemented. So there’s no duplication of commands to agents. Each agent, upon receiving a task, will produce at most one response (they either succeed, fail and send an error, or crash and send nothing; if crash, orchestrator times out). Therefore, orchestrator will receive at most one response per task. Combined with the counting mechanism, it will mark that task done and not expect a second. If somehow two responses came (say an agent erroneously sent twice), the second one would find `count` already 0 or negative and be essentially ignored. This ensures no double counting or confusion.

**3. Partial Order Preservation:** If the LLM designates certain tasks to be sequential (because one’s output is needed for another), the orchestrator can handle that by dispatching in sequence or explicitly waiting between tasks. Our pseudocode doesn’t show dependent tasks explicitly, but we could implement it: e.g., if the plan says task2 depends on result of task1, orchestrator could dispatch task1, then in `_handle_task_response` for task1, trigger task2. This would maintain the needed order. For now, we assume tasks are either independent or the LLM will include any needed combination itself (like instruct an agent to do multiple steps). The important part: **no inherent race conditions**. Independent tasks may complete in any order, which is fine; orchestrator only finalizes when all are in. If tasks had an order, orchestrator would enforce it by not dispatching the later one too early. Thus, causality intended by the LLM plan can be preserved. And since AMCP events preserve ordering per topic (if using same partition, etc.) we can rely on that if needed for sequential chain, or use separate events to ensure clarity.

**4. Resource Management:** Each request’s data is stored in `pending_tasks` for the duration of that request. This structure is cleaned up in `_finalize_request`. Therefore, no memory leak per request. The orchestrator agent itself is long-lived, but its per-request footprint is bounded by the size of results aggregated. Specialist agents similarly handle one event at a time and do not retain large state after responding (unless their function inherently caches something, but that’s within their domain).
- The LLM calls are stateless (other than perhaps Ollama’s internal caching), so no accumulating state there either.
- If the orchestrator crashed mid-orchestration (e.g., its process goes down), obviously that particular request would fail. But AMCP could be configured for orchestrator agent restart or a standby orchestrator picks up new requests. The system is as resilient as the underlying platform – we haven’t introduced a new single point of failure beyond what orchestrator itself represents. In practice, one can run multiple orchestrators behind a broker and route requests round-robin for HA (though coordinating the registry between them would be needed, maybe via the dedicated RegistryAgent approach).

**5. Security and Permission Safety:** The orchestrator will carry along user context (as shown, we attach `user_meta` to tasks) so that specialist agents know who initiated the action. In an enterprise setting, each agent could enforce authorization (e.g., a Finance DB Agent might verify the user has finance report access before running a query). The orchestrator can act as a policy gatekeeper too: if an LLM tries to invoke a capability the user isn’t allowed to use (based on roles), the orchestrator can detect that from `user_meta` and skip or deny that task. This prevents the system from performing unauthorized actions just because the LLM “thought of it.” By including user identity and scopes in every event, we enable such checks. These measures ensure that the orchestrated actions adhere to security constraints (no agent will execute a request without the proper context). **Safety from misuse by LLM** is a new consideration – effectively the LLM is like a planner that could try something unintended. We mitigate this by constraining available `capabilities` (the LLM can only choose from what we advertise, e.g., it cannot spontaneously call a nonexistent "DeleteDatabase" capability if we never list it)[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html), and by validating each chosen capability against policies. In worst case, if the LLM suggests an unpermitted action, orchestrator will not execute it (the pseudocode simply wouldn’t find an agent, or we add an explicit policy check).

**6. Formal Verification Considerations:** The orchestrator’s logic closely resembles a finite-state workflow for each request: initial state (tasks not dispatched), after dispatch (waiting for N tasks), and final state (responded). We could model each request’s progress as a state machine:
- **State 0:** Request received (N tasks identified, 0 results ready).
- **State 1:** Partial results gathered (k out of N done, N-k pending).
- ...
- **State N:** All results ready or some failed/timed out.
- **State Final:** Response sent.

We ensure there’s no transition loop: states progress monotonically towards Final. Once in Final, the request is removed. If the orchestrator receives a task response for a request not in its dictionary (which would be an event for an already finalized or unknown request), it ignores it, so no state reactivation. This prevents any possibility of re-processing or inconsistent state.

**Edge Cases:** 
- If the LLM returns an empty plan (perhaps it decides it can answer directly or finds nothing to do): our design handles it by either using LLM to directly answer or sending a trivial response. So user still gets an answer (maybe a friendly "I don't know" response if no plan).
- If a specialist agent crashes after completing work but before sending response (so orchestrator waits forever): our timeout will trigger and orchestrator can mark that subtask failed and still return whatever partial info it has. This ensures the user isn’t left hanging. Such resilience can be improved with retries (e.g., dispatch to a different agent capability if available), but that’s an enhancement.

In summary, the design is **safe from deadlocks** (no cyclic waits), **tolerant to missing responses** (via timeouts and default error behavior), and **guarantees exactly one end response per request**. The use of events and correlation aligns with proven messaging patterns for workflow orchestration, so we inherit their reliability principles. Moreover, by keeping the orchestrator logic single-threaded within its agent, we avoid multi-threading bugs – concurrency is managed by the event queue and the natural isolation of agents.

One potential risk is the **LLM output quality**: if it plans incorrectly, the system will still try to execute it. This isn't a safety issue for the platform per se (more a functional accuracy issue). We mitigate negatives by bounding what can happen (the orchestrator won’t execute unknown or disallowed tasks), so the worst case is an incorrect or incomplete answer, not a system failure or security breach. Over time, the plan prompts can be refined or even the LLM fine-tuned to improve reliability, but that’s outside the protocol’s correctness (it’s about performance/quality).

To solidify confidence, one could apply formal methods to the orchestrator’s core (minus the LLM). For example, model the request handling as a Petri net or use temporal logic: ensure that eventually (response_sent) holds for each request event received. Given the structured approach above, such verification is feasible on the pseudo-code ignoring the LLM call (treat `generatePlan` as nondeterministic but yielding a finite list of tasks). The design’s alignment with known patterns (it’s essentially a broker-mediated request/reply with a coordinator) further supports its soundness[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).

## Testing Strategy

To validate the implementation, we propose a multi-layered testing approach: **unit tests** for individual methods, **integration tests** for orchestrated flows, and a **demo scenario** to simulate end-to-end usage.

### Unit Tests

**OrchestratorAgent Unit Tests:** We can mock dependencies (LLM connector and registry) to test orchestrator logic in isolation.

1. *Test plan dispatch:* Simulate a user request event where `generatePlan` returns a known plan. For example, user asks "What’s the weather and stock price?" and we mock `llm.generatePlan` to return `[{"capability": "weather.get", "params": {"location": "NYC"}}, {"capability": "stocks.get", "params": {"ticker": "AMZN"}}]`. Also, set `capabilities` map to have entries for "weather.get" and "stocks.get" (pointing to dummy agent IDs). Then call `_handle_user_request`. Verify that:
   - `pending_tasks` now has an entry for that request ID with count=2.
   - Two events were published (we could spy on `publish` method or have it append to a list) with correct type `task.request` and appropriate data (NYC, AMZN) and correlation equal to request ID.
   - If we then call `_handle_task_response` twice with constructed events for those tasks, ensure it aggregates and calls `_finalize_request`. We might need to mock `llm.generateAnswer` for the final composition. Check that `user.response` event is published with correlation matching input and combined info in answer.
   - Also check that `pending_tasks` entry is removed after finalization.

2. *Test no-capability scenario:* Mock a plan that includes a capability not in registry. For example, plan has `{"capability": "unknown.skill"}` but `capabilities` has no such key. Run `_handle_user_request` and verify:
   - The task count is decremented for the missing skill and no event published for it.
   - If that was the only task, the orchestrator finalizes immediately with some error indication. We expect a `user.response` with an answer perhaps saying "could not complete". In absence of actual LLM, our code might just send a generic answer or blank – which is fine for test, we just ensure response is sent.
   
3. *Test sequential dependency:* If we support sequential tasks in plan, we’d test that logic. For instance, plan could encode "first do A then do B". We would ensure orchestrator doesn’t dispatch B until A’s result comes. (This requires a slight alteration in code to handle dependency, which we haven’t explicitly coded. If we add it, we test it accordingly.)

4. *Test health check hooks:* If orchestrator uses `isHealthy` or has any error handling (not shown above but could be added), we test that misbehavior triggers expected logs or signals. E.g., simulate `llm.generatePlan` raising an exception (LLM service down), ensure orchestrator catches it and sends a failure response, rather than crashing.

**Specialist Agent Unit Tests:** These are simpler – test that given a task event, the agent produces the correct output event.

- For example, a `ReportGenAgent` that takes data and produces a report summary. We feed it a fake `task.request` event and either inspect the returned event from its handler or if it uses `publish`, we intercept that. Check the event’s data is correct (e.g., contains a "report" field). Also verify correlation propagated.
- Test error handling in agent: e.g., feed it a task event with malformed data that causes an exception in its logic. The agent should catch exceptions and return an `error` in the response data, not crash. Confirm that behavior by simulating the exception and seeing the output event.

**LLM Connector Unit Tests:** This can be done by mocking the HTTP call.

- Test that given a certain prompt, the connector forms the proper HTTP request (maybe using a Requests-mock library to capture outgoing calls).
- Test handling of API errors: if Ollama returns a non-200 or times out, our connector should raise an exception which we ensure orchestrator would catch (in a combined integration test perhaps).
- Because Ollama is local, integration tests could run against a real instance with a small model to see end-to-end.

### Integration Tests

We set up a minimal AMCP environment in memory with:
- One OrchestratorAgent instance.
- A couple of dummy specialist agents.
- A simple in-process event broker (AMCP’s in-memory broker for test).

Then we simulate a full scenario by sending an event into the orchestrator and observing final output. This can be done by directly calling orchestrator’s `handle_event` (since in tests we control scheduling) or by actually running the agents in threads.

**Integration Test Scenario:** “Weather and Joke”. Suppose we have two specialist agents: WeatherAgent and JokeAgent. The user asks: "What's the weather in Paris and tell me a programming joke?". The orchestrator should plan two tasks and get answers.

- We configure WeatherAgent to respond with a fixed “sunny” for any location (for test), and JokeAgent with a fixed joke.
- We mock the LLM to output a plan like: `[{"capability": "weather.get", "params": {"city": "Paris"}}, {"capability": "joke.get", "params": {"topic": "programming"}}]`. (Alternatively, run a real small model with a prompt that likely yields that plan).
- We inject a `user.request` event. Then either step through orchestrator then manually call the two agents (simulate the broker delivering tasks), or truly run them concurrently.
- Finally, validate that we got a `user.response` event containing both a weather answer and a joke. Possibly the orchestrator’s final synthesis just concatenates them (depending on prompt or logic). We check that outcome contains e.g. "In Paris, it's sunny. Here is a programming joke: ...".

**System/Performance Test:** For concurrency and performance, we could simulate multiple requests in quick succession and see that none of them interfere (each returns correct answer to correct user). This would reveal any mishandling of correlation or state bleed. We might also measure how adding more agents scales (e.g., tasks truly run in parallel).

### Demo Scenario (Use Case Example)

Let’s walk through an **end-to-end example** in a narrative form, which can also serve as a manual integration test:

**Use Case:** *Expense Report Generation* – similar to the example earlier but now fully demonstrating the design.

- **Initial Setup:** Three specialist agents are running: 
  1. `<AgentDB>` – responds to finance database queries (capability: `"finance.db.query"`).
  2. `<AgentAnalyze>` – calculates statistics (capability: `"finance.analyze.expenses"`).
  3. `<AgentReport>` – generates a formatted report (capability: `"document.report.generate"`).
  The OrchestratorAgent is running and has registered these capabilities in its map (via their `agent.register` events which were sent on startup).
- **User Request:** A user triggers a request (via Gateway or test script): *"Produce an expense summary report for Q4 2024."* This becomes a `user.request` event with `id = REQ123` (for reference) and data `{"query": "Produce an expense summary report for Q4 2024", "user": "alice", "reqId": "REQ123"}`.
- **Orchestration Plan:** Orchestrator receives this event (REQ123). It prompts the LLM: *"User asks: 'Produce an expense summary report for Q4 2024'. Available skills: [finance.db.query, finance.analyze.expenses, document.report.generate]. Plan the steps."* The LLM returns a plan, say in JSON:
  ```json
  [
    {"capability": "finance.db.query", "params": {"quarter": "Q4 2024"}},
    {"capability": "finance.analyze.expenses", "params": {"metric": "summary by department"}},
    {"capability": "document.report.generate", "params": {"format": "PDF"}}
  ]
  ```
  This indicates:
   1. Query the finance DB for Q4 2024 raw expenses.
   2. Analyze the expenses to produce a summary by department.
   3. Generate a PDF report of the summary.
- **Task Dispatch:** The orchestrator sets up `pending_tasks[REQ123] = {"count": 3, ...}`. It then publishes three events:
  - Event A: `task.request` (id T1, correlationId REQ123, subject `finance.db.query`, data `{"quarter": "Q4 2024"}`) → delivered to `<AgentDB>` (since registry says that capability goes to AgentDB).
  - Event B: `task.request` (id T2, correlationId REQ123, subject `finance.analyze.expenses`, data `{"metric": "summary by department"}`) → delivered to `<AgentAnalyze>`.
  - Event C: `task.request` (id T3, correlationId REQ123, subject `document.report.generate`, data `{"format": "PDF"}`) → delivered to `<AgentReport>`.
  Each event is like a separate envelope with the same correlationId tying them to REQ123.
- **Agent Execution:** 
  - `<AgentDB>` receives Event A. It runs the query for Q4 2024. Suppose it fetches raw expense records `[{"dept": "Sales", "amount": 10000}, {"dept": "Engineering", "amount": 8000}, ...]`. It then publishes `task.response` (correlationId REQ123) with data `{"raw_data": [...records...]}`[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).
  - `<AgentAnalyze>` receives Event B. It doesn’t have the raw data yet! Here is a subtlety: the plan assumed analysis can happen in parallel, but realistically, analysis needs data from DB. The LLM might not realize dependency or might implicitly rely on some data source. This is a design decision: We *could* have orchestrator detect that AgentAnalyze needs DB output. How? Possibly if the plan had an ordering, or if the agent itself upon receiving no data might wait or request it. A simpler approach is to refine the plan: instruct LLM to output tasks with correct dependencies. If not, orchestrator can orchestrate iteratively: first run DB, then feed result to analyze. But to keep this demo flowing, let’s assume AgentAnalyze knows to fetch data itself (maybe it queries the DB too, or orchestrator should have provided raw data as param). More proper: orchestrator could have piped the DB result into the analyze task param. For demonstration, assume orchestrator, after getting DB result, triggers analyze with that data (sequential approach). So actually, orchestrator might not dispatch analysis until DB returns. We can adapt: orchestrator sees plan tasks, notices that "finance.analyze.expenses" likely needs input from DB, so it decides to do sequential dispatch: send DB first, and put others on hold until DB returns. (This logic isn’t in pseudocode but could be a feature.)
  - To avoid confusion, let’s say the LLM plan was smarter: it combined step 1 and 2: the DB agent could be instructed to both fetch and summarize (if it had such a capability), or orchestrator itself after DB result calls analyze. Either way, eventually orchestrator gets a summary. For brevity: Orchestrator gets DB’s raw data response, then manually invokes analyze agent with that data (not exactly as we coded, but plausible extension). AgentAnalyze then returns a summary: `{"summary": {"Sales": 10000, "Engineering": 8000,...}, "total": 18000}`.
  - `<AgentReport>` receives Event C. It likely needs the summary to generate the report. Again, the orchestrator could combine results: perhaps orchestrator waited to send report task until analysis is done (which a sequential plan would dictate). So assume after analysis, orchestrator sends the report task with the summary attached in params. AgentReport generates a PDF or a link, and replies with `{"url": "https://files/report_Q4_2024.pdf"}`.
- **Aggregation:** Orchestrator collects responses. Let’s say at the end we have:
  - From AgentDB: raw data (used then discarded or stored).
  - From AgentAnalyze: summary data.
  - From AgentReport: report URL.
  Now `pending_tasks[REQ123]["count"]` goes to 0. Orchestrator triggers finalization.
- **Final Response Composition:** Orchestrator calls `llm.generateAnswer` with a prompt like: *"Sales: \$10k, Engineering: \$8k, Total: \$18k. A PDF report is available at the provided URL. Draft a response for the user."* The LLM might produce: *"The total expenses for Q4 2024 were \$18k (Sales: \$10k, Engineering: \$8k). A detailed report has been generated: https://files/report_Q4_2024.pdf."* This text is put into `user.response` event.
- **Result Delivery:** Gateway agent sees `user.response` for REQ123 (correlation). It forwards the content to Alice (maybe via HTTP response or chat message).
- **Verification:** Alice receives the answer with the data and link, fulfilling her request.

This scenario tests a variety of things: multi-step orchestration, dependent tasks, combining data, etc. It exposed that our simple parallel plan needed some sequential handling for meaningful results. We can adjust the design to allow orchestrator to do sequential subtasks: e.g., have the LLM indicate dependencies or simply handle it manually as above (first wait for DB then call analysis, etc.). This is a refinement but not a fundamental flaw. It shows the need for either more sophisticated planning or letting orchestrator inject results into subsequent prompts. For now, we assume either the LLM will plan sequentially (possibly by listing tasks in order and flagging that second needs first’s output) or we handle it with multiple rounds.

A simpler fully parallel scenario (like the weather + joke example) has no such dependency and flows without issue. That’s why in testing we’d include both independent and dependent workflows.

### Test Scripts and Tools

We can automate tests using a combination of JUnit (for Java, if AMCP is Java) or PyTest (if a Python harness with simulated events). But since this is design level, a script could be:

- **Script to run demo:** Provide a small program `demo_orchestration.py` (or Java main) that:
  - Instantiates an AgentContext and the orchestrator agent; registers it.
  - Instantiates a few specialist agents (maybe dummy ones that reply with hardcoded data for simplicity).
  - Starts the context’s event loop (if needed).
  - Sends a `user.request` event (either via context API or directly calling orchestrator’s handler).
  - Listens for `user.response` event or just prints output stored by gateway agent.

For example, a Python-like pseudo-test:
```python
# Set up context and agents
context = AgentContext()
gateway = DummyGatewayAgent()  # simplify: this will catch responses
orchestrator = OrchestratorAgent()
agent1 = WeatherAgent()
agent2 = JokeAgent()
context.register_agents([gateway, orchestrator, agent1, agent2])
context.start()

# Simulate a user query via gateway:
gateway.simulate_user_query("What's the weather in Paris and a programming joke?")
# The DummyGatewayAgent internally publishes a user.request event and waits for response.
# We then wait a moment and check gateway for output.
time.sleep(2)  # wait for processing (in real test, better use event sync)
print("Response to user:", gateway.last_response)
context.stop()
```

The output printed should contain both a weather report and a joke, showing orchestrator worked.

**Client Command-Line Example:** 
If we provide a CLI, it could be as simple as:
```bash
$ amcp_orchestrator_client "Generate an expense report for Q4 2024"
```
This command (which could be a wrapper that sends an event and waits) might output:
```
The total expenses for Q4 2024 were $18k (Sales: $10k, Eng: $8k).
A detailed PDF report has been generated: [Report_Q42024.pdf].
```
Alternatively, using `curl` to a gateway’s HTTP endpoint:
```bash
$ curl -X POST -H "Content-Type: application/json" -d '{"query": "expense report for Q4 2024"}' http://localhost:8080/ask
```
The gateway would translate that to a `user.request` and eventually return the JSON/text answer from `user.response`.

## Conclusion

This technical design details how to extend AMCP v1.5 with a powerful orchestrator capability, marrying event-driven agent coordination with LLM-based planning. We covered class structures, with the **OrchestratorAgent** at the core, interacting with specialist agents purely through AMCP events. We presented pseudocode for the orchestrator and agents, demonstrating **how the system functions under the hood** – from receiving a request to returning a result – and ensuring important properties like correctness, fairness, and security.

By following this design, developers can implement the orchestrator feature as an add-on module to AMCP. The design stays true to AMCP’s principles (asynchronous pub/sub, loose coupling, CloudEvents compliance[3](https://amadeusworkplace-my.sharepoint.com/personal/xcallens_amadeus_com/Documents/ailored%20to%20specific%20event.docx?web=1)[3](https://amadeusworkplace-my.sharepoint.com/personal/xcallens_amadeus_com/Documents/ailored%20to%20specific%20event.docx?web=1)) while significantly enhancing the expressive power of the agent mesh. We effectively enable **goal-directed orchestration**: you ask in plain language, and the agents collectively respond, each doing what it does best, coordinated by the LLM’s “reasoning”.

Finally, rigorous testing has been outlined to validate each part of the system. Once implemented and tested, this extension would allow AMCP-based systems to tackle far more complex tasks than before, leveraging multiple AI and non-AI capabilities in concert. It transforms the AMCP v1.5 mesh into an intelligent **agent society** with an LLM “brain” guiding the teamwork[1](https://microsoft.github.io/multi-agent-reference-architecture/docs/reference-architecture/Patterns.html).

