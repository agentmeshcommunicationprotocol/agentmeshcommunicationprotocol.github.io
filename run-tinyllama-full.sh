#!/bin/bash

# Full OLLAMA TinyLlama Demo Runner with Build for AMCP v1.5
echo "üöÄ AMCP OLLAMA TinyLlama 1.1B Demo (Full Build)"
echo "=============================================="

# Set JAVA_HOME
export JAVA_HOME="/opt/homebrew/Cellar/openjdk@21/21.0.8/libexec/openjdk.jdk/Contents/Home"

# Check OLLAMA service
echo "üîç Checking OLLAMA service..."
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚ùå OLLAMA not running. Starting OLLAMA service..."
    ollama serve &
    sleep 3
    echo "‚úÖ OLLAMA service started"
fi

# Check TinyLlama model
echo "üîç Checking TinyLlama model..."
if ! ollama list | grep -q tinyllama; then
    echo "üì• Downloading TinyLlama model..."
    ollama pull tinyllama
fi

echo "‚úÖ All prerequisites ready!"
echo ""

# Build project if needed
if [ ! -d "connectors/target/classes" ] || [ ! -d "core/target/classes" ]; then
    echo "üî® Building project..."
    mvn clean compile -q
    if [ $? -ne 0 ]; then
        echo "‚ùå Build failed"
        exit 1
    fi
    echo "‚úÖ Build completed"
fi

echo "üí° TinyLlama 1.1B Specifications:"
echo "   ‚Ä¢ Parameters: 1.1 billion"
echo "   ‚Ä¢ Download Size: ~637MB"
echo "   ‚Ä¢ RAM Usage: ~2-3GB"
echo "   ‚Ä¢ Speed: Very fast (small model size)"
echo "   ‚Ä¢ Trained on: 1+ trillion tokens"
echo "   ‚Ä¢ Perfect for: Basic conversation, lightweight AI tasks"
echo ""
echo "üéØ Available Commands in Demo:"
echo "   ‚Ä¢ Type any message to chat with TinyLlama"
echo "   ‚Ä¢ /help - Show available commands"
echo "   ‚Ä¢ /model - Display model information"
echo "   ‚Ä¢ /exit - Exit the demo"
echo ""
echo "üöÄ Starting OLLAMA Integration Demo..."
echo "====================================="

# Run with Maven to ensure all dependencies are available
cd connectors && mvn exec:java -Dexec.mainClass=io.amcp.connectors.ollama.OllamaIntegrationDemo -q